# ğŸ“Š IPC Argentina - Automated Data Pipeline

> **ETL pipeline automatizado para anÃ¡lisis de inflaciÃ³n argentina con modelo dimensional en PostgreSQL**

Pipeline end-to-end que extrae datos del Ãndice de Precios al Consumidor (IPC) desde la API pÃºblica de datos.gob.ar, los transforma a un modelo Star Schema y los carga en PostgreSQL con actualizaciones incrementales mensuales.

## ğŸ¯ Objetivo

Centralizar datos histÃ³ricos de inflaciÃ³n argentina en una base de datos relacional optimizada para anÃ¡lisis multidimensional, permitiendo calcular mÃ©tricas clave (MoM, YoY, incidencias) y generar insights sobre tendencias econÃ³micas por regiÃ³n y categorÃ­a.

---

## ğŸ—ï¸ Arquitectura del Sistema

```mermaid
graph LR
    A[datos.gob.ar API<br/>INDEC] -->|HTTP GET<br/>CSV| B[Python ETL<br/>Pandas + Requests]
    B -->|SQLAlchemy<br/>UPSERT| C[PostgreSQL<br/>Supabase]
    C -->|SQL Queries| D[Power BI<br/>Tableau<br/>Streamlit]
    
    style A fill:#e1f5ff
    style B fill:#fff4e1
    style C fill:#e8f5e9
    style D fill:#f3e5f5
```

**Flujo de datos:**
1. **ExtracciÃ³n:** Descarga automÃ¡tica desde API pÃºblica (3 endpoints CSV)
2. **TransformaciÃ³n:** NormalizaciÃ³n Wideâ†’Long + Parsing de metadata
3. **Carga:** Modelo Star Schema con updates incrementales
4. **VisualizaciÃ³n:** Dashboards conectados vÃ­a SQL queries

---

## âš™ï¸ Highlights del Pipeline ETL

### **ExtracciÃ³n**
- Consumo de API REST pÃºblica (datos.gob.ar) con manejo de errores y timeouts
- ValidaciÃ³n de estructura de datos antes de procesamiento
- DetecciÃ³n automÃ¡tica de nuevos perÃ­odos disponibles

### **TransformaciÃ³n**
- **Unpivot:** ConversiÃ³n de formato Wide a Long usando `pandas.melt()`
- **Parsing:** ExtracciÃ³n de metadata (regiÃ³n, categorÃ­a) desde nombres de columnas
- **Limpieza:** EliminaciÃ³n de nulls, normalizaciÃ³n de fechas, validaciÃ³n de tipos

### **Carga**
- **LÃ³gica incremental:** `ON CONFLICT ... DO UPDATE` para prevenir duplicados
- **Modelo dimensional:** Star Schema con tablas fact/dimension optimizadas
- **Ãndices:** OptimizaciÃ³n de queries temporales (`idx_fact_fecha`, `idx_fact_region`)

### **OrquestaciÃ³n**
- Scripts programados (Cron / GitHub Actions) para ejecuciÃ³n mensual automÃ¡tica
- Logs detallados con mÃ©tricas de inserciÃ³n/actualizaciÃ³n
- Manejo de revisiones retroactivas del INDEC (descarga Ãºltimos 2 meses)

---

## ğŸ“ Modelo de Datos - Star Schema

```sql
-- Tablas de DimensiÃ³n
CREATE TABLE dim_region (
    region_id SERIAL PRIMARY KEY,
    region_nombre VARCHAR(50) UNIQUE
);

CREATE TABLE dim_categoria (
    categoria_id SERIAL PRIMARY KEY,
    categoria_nombre VARCHAR(100) UNIQUE,
    clasificacion VARCHAR(50)
);

-- Tabla de Hechos
CREATE TABLE fact_inflacion (
    fecha DATE,
    region_id INT REFERENCES dim_region(region_id),
    categoria_id INT REFERENCES dim_categoria(categoria_id),
    valor_indice DECIMAL(18, 4),
    PRIMARY KEY (fecha, region_id, categoria_id)
);

-- Ãndices
CREATE INDEX idx_fact_fecha ON fact_inflacion(fecha);
CREATE INDEX idx_fact_region ON fact_inflacion(region_id);
CREATE INDEX idx_fact_categoria ON fact_inflacion(categoria_id);
```

**Granularidad:** Mensual | **PerÃ­odo:** Dic 2023 â†’ Presente | **Registros:** ~30,000+

---

## ğŸš€ CÃ³mo Correr el Proyecto

### **Prerequisitos**
- Python 3.8+
- PostgreSQL (Supabase recomendado)
- Git

### **1. Clonar repositorio**
```bash
git clone https://github.com/tu-usuario/ipc-argentina-pipeline.git
cd ipc-argentina-pipeline
```

### **2. Crear entorno virtual**
```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate
```

### **3. Instalar dependencias**
```bash
pip install -r requirements.txt
```

### **4. Configurar variables de entorno**

Crear archivo `.env` basado en `.env.example`:

```bash
cp .env.example .env
```

Editar `.env` con tus credenciales:
```env
DB_USER=postgres.tu_project_id
DB_PASSWORD=tu_contraseÃ±a_segura
DB_HOST=aws-0-us-west-2.pooler.supabase.com
DB_PORT=6543
DB_NAME=postgres
START_DATE=2023-12-01
```

### **5. Ejecutar carga inicial**

```bash
# Paso 1: Descargar datos
python ipc_scraper.py

# Paso 2: Crear estructura de BD y cargar datos
python db_setup_secure.py
```

Salida esperada:
```
âœ… Estructura de base de datos verificada/creada
âœ… Proceso completado! Datos sincronizados
   Total registros: 28,450
   PerÃ­odo: 2023-12-01 a 2025-02-01
```

### **6. ActualizaciÃ³n mensual**

```bash
python update_monthly.py
```

Salida esperada:
```
ğŸ“… Ãšltima fecha en DB: 2025-01-01
ğŸ“¥ Descargando datos desde: 2024-11-01
ğŸ“Š Datos nuevos encontrados: 1,250 registros
âœ… ACTUALIZACIÃ“N COMPLETADA
   Registros insertados: 1,200
   Registros actualizados: 50
```

---

## ğŸ“Š Consultas SQL - Window Functions

### **VariaciÃ³n Mensual (MoM) - Month over Month**

```sql
WITH inflacion_mensual AS (
    SELECT 
        f.fecha,
        f.valor_indice,
        LAG(f.valor_indice) OVER (ORDER BY f.fecha) as valor_mes_anterior
    FROM fact_inflacion f
    JOIN dim_region r ON f.region_id = r.region_id
    JOIN dim_categoria c ON f.categoria_id = c.categoria_id
    WHERE r.region_nombre = 'Nacional'
      AND c.categoria_nombre = 'Nivel General'
      AND c.clasificacion = 'Total'
)
SELECT 
    fecha,
    valor_indice as indice_actual,
    ROUND(((valor_indice / valor_mes_anterior - 1) * 100), 2) as variacion_mom_pct
FROM inflacion_mensual
WHERE valor_mes_anterior IS NOT NULL
ORDER BY fecha DESC
LIMIT 12;
```

### **VariaciÃ³n Interanual (YoY) - Year over Year**

```sql
WITH inflacion_yoy AS (
    SELECT 
        f.fecha,
        f.valor_indice,
        LAG(f.valor_indice, 12) OVER (ORDER BY f.fecha) as valor_anio_anterior
    FROM fact_inflacion f
    JOIN dim_region r ON f.region_id = r.region_id
    JOIN dim_categoria c ON f.categoria_id = c.categoria_id
    WHERE r.region_nombre = 'Nacional'
      AND c.categoria_nombre = 'Nivel General'
)
SELECT 
    fecha,
    valor_indice,
    ROUND(((valor_indice / valor_anio_anterior - 1) * 100), 2) as variacion_yoy_pct
FROM inflacion_yoy
WHERE valor_anio_anterior IS NOT NULL
ORDER BY fecha DESC
LIMIT 12;
```

---

## ğŸ“Š Business Intelligence & Analytics

El modelo Star Schema estÃ¡ optimizado para conectarse con herramientas de visualizaciÃ³n:

### **KPIs Disponibles**

- **VariaciÃ³n MoM (Month over Month):** InflaciÃ³n del Ãºltimo mes
- **VariaciÃ³n YoY (Year over Year):** ComparaciÃ³n interanual
- **InflaciÃ³n Acumulada:** Desde inicio del aÃ±o o perÃ­odo especÃ­fico
- **Incidencia por Rubro:** QuÃ© categorÃ­as explican mÃ¡s la inflaciÃ³n total
- **AnÃ¡lisis Core vs No-Core:** NÃºcleo, Regulados y Estacionales
- **Disparidad Regional:** ComparaciÃ³n entre GBA, Pampeana, NOA, NEA, Cuyo, Patagonia

### **Herramientas de VisualizaciÃ³n**

| Herramienta | ConexiÃ³n | Casos de Uso |
|-------------|----------|--------------|
| **Power BI** | PostgreSQL Connector | Dashboards ejecutivos, reportes automÃ¡ticos |
| **Tableau** | Native PostgreSQL | AnÃ¡lisis ad-hoc, storytelling visual |
| **Streamlit** | SQLAlchemy | Aplicaciones web interactivas |
| **Python (Pandas)** | psycopg2 / SQLAlchemy | AnÃ¡lisis exploratorio, notebooks |

### **Ejemplo de Dashboard**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ InflaciÃ³n MoM   â”‚  â”‚ InflaciÃ³n YoY   â”‚  â”‚ Acumulado 2025  â”‚
â”‚     2.7%        â”‚  â”‚    117.5%       â”‚  â”‚     2.7%        â”‚
â”‚   â–² +0.3 pp     â”‚  â”‚   â–¼ -7.7 pp     â”‚  â”‚   â–² +2.7 pp     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Top 5 CategorÃ­as por Incidencia:
Alimentos y bebidas      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 1.2pp
Transporte              â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.5pp
Vivienda                â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.4pp
Salud                   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.3pp
RecreaciÃ³n              â–ˆâ–ˆâ–ˆâ–ˆ 0.2pp
```

---

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

```bash
# 1. Clonar repositorio
git clone https://github.com/tu-usuario/ipc-argentina-pipeline.git
cd ipc-argentina-pipeline

# 2. Crear entorno virtual e instalar dependencias
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

# 3. Configurar credenciales (crear archivo .env)
cp .env.example .env
# Editar .env con tus credenciales de Supabase

# 4. Carga inicial
python ipc_scraper.py              # Descarga datos
python db_setup_secure.py          # Crea estructura y carga

# 5. ActualizaciÃ³n mensual (automatizar con cron/GitHub Actions)
python update_monthly.py
```

---

## ğŸ› ï¸ Stack TecnolÃ³gico

| Componente | TecnologÃ­a |
|------------|------------|
| **Lenguaje** | Python 3.10+ |
| **ETL** | Pandas, Requests |
| **Base de Datos** | PostgreSQL (Supabase) |
| **ORM** | SQLAlchemy |
| **OrquestaciÃ³n** | GitHub Actions / Cron |
| **BI Tools** | Power BI, Tableau, Streamlit |

---

## ğŸ“š Referencias

- [INDEC - MetodologÃ­a IPC](https://www.indec.gob.ar/indec/web/Nivel4-Tema-3-5-31)
- [datos.gob.ar - Dataset IPC](https://datos.gob.ar/dataset/sspm-indice-precios-consumidor-nacional-ipc-nivel-general-categorias)
- [Supabase Documentation](https://supabase.com/docs)
